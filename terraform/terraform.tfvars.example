# =============================================================================
# Proxmox + Talos + Omni Configuration
# =============================================================================
# Copy this file: cp terraform.tfvars.example terraform.tfvars
# Edit terraform.tfvars with your actual values
# =============================================================================

# Proxmox Server Configuration
proxmox_servers = {
  "pve1" = {
    api_url          = "https://192.168.10.160:8006/api2/json"
    api_token_id     = "terraform@pve!terraform"
    api_token_secret = "YOUR_PROXMOX_API_TOKEN_HERE"  # Replace with actual token
    node_name        = "hp-server-1"
    tls_insecure     = true

    # Storage configuration
    storage_control_plane_os   = "local-lvm"
    storage_control_plane_data = ""           # No data disk for control planes
    storage_worker_os          = "local-lvm"
    storage_worker_data        = "local-lvm"  # All workers get secondary storage
    storage_gpu_worker_os      = "local-lvm"
    storage_gpu_worker_data    = "local-lvm"  # GPU workers get secondary storage
    network_bridge             = "vmbr0"
  }
}

# Network Configuration
network_config = {
  subnet      = "192.168.10.0/24"
  gateway     = "192.168.10.1"
  dns_servers = ["1.1.1.1", "1.0.0.1"]
  vlan_id     = 0
}

# Talos Configuration
talos_iso     = "local:iso/talos-1.11.5.iso"
talos_gpu_iso = "local:iso/talos-1.11.5-gpu.iso"
talos_version = "v1.11.5"
cluster_name  = "talos-prod-cluster"

# =============================================================================
# Control Plane Nodes (3 nodes, no secondary storage)
# =============================================================================
# MAC Pattern: BC:24:11:01:00:XX
# IPs: 192.168.10.100-102
# VMIDs: 100-102

control_planes = [
  {
    name              = "talos-control-1"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.100"
    mac_address       = "BC:24:11:01:00:00"
    cpu_cores         = 4
    memory_mb         = 8192
    os_disk_size_gb   = 70
    data_disk_size_gb = 0  # No secondary disk
    storage_os_override   = ""  # Empty = use default from proxmox_servers
    storage_data_override = ""
  },
  {
    name              = "talos-control-2"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.101"
    mac_address       = "BC:24:11:01:00:01"
    cpu_cores         = 4
    memory_mb         = 8192
    os_disk_size_gb   = 70
    data_disk_size_gb = 0
    storage_os_override   = ""
    storage_data_override = ""
  },
  {
    name              = "talos-control-3"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.102"
    mac_address       = "BC:24:11:01:00:02"
    cpu_cores         = 4
    memory_mb         = 8192
    os_disk_size_gb   = 70
    data_disk_size_gb = 0
    storage_os_override   = ""
    storage_data_override = ""
  }
]

# =============================================================================
# Worker Nodes (3 nodes, all with 250GB Longhorn storage)
# =============================================================================
# MAC Pattern: BC:24:11:02:00:XX
# IPs: 192.168.10.110-112
# VMIDs: 110-112

workers = [
  {
    name              = "talos-worker-1"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.110"
    mac_address       = "BC:24:11:02:00:00"
    cpu_cores         = 8
    memory_mb         = 16384
    os_disk_size_gb   = 70
    data_disk_size_gb = 250  # Longhorn storage
    storage_os_override   = ""  # You can override per-VM by setting storage name here
    storage_data_override = ""  # Example: "nvme-pool" or "ceph-storage"
  },
  {
    name              = "talos-worker-2"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.111"
    mac_address       = "BC:24:11:02:00:01"
    cpu_cores         = 8
    memory_mb         = 16384
    os_disk_size_gb   = 70
    data_disk_size_gb = 250  # Longhorn storage
    storage_os_override   = ""
    storage_data_override = ""
  },
  {
    name              = "talos-worker-3"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.112"
    mac_address       = "BC:24:11:02:00:02"
    cpu_cores         = 8
    memory_mb         = 16384
    os_disk_size_gb   = 70
    data_disk_size_gb = 250  # Longhorn storage
    storage_os_override   = ""
    storage_data_override = ""
  }
]

# =============================================================================
# GPU Worker Nodes (1 node with 250GB Longhorn storage)
# =============================================================================
# MAC Pattern: BC:24:11:03:00:XX
# IPs: 192.168.10.115
# VMIDs: 120

gpu_workers = [
  {
    name              = "talos-gpu-worker-1"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.115"
    mac_address       = "BC:24:11:03:00:00"
    cpu_cores         = 16
    memory_mb         = 32768
    os_disk_size_gb   = 70
    data_disk_size_gb = 250  # Longhorn storage
    gpu_pci_id        = "01:00"  # For documentation only (using mapped resource)
    storage_os_override   = ""
    storage_data_override = ""
  }
]

# VM Options
vm_start_on_boot = true
vm_qemu_agent    = true
vm_protection    = false

# =============================================================================
# NOTES
# =============================================================================
#
# MAC Addresses (predictable pattern):
# - Control planes: BC:24:11:01:00:XX
# - Workers: BC:24:11:02:00:XX
# - GPU workers: BC:24:11:03:00:XX
#
# Disk Configuration:
# - OS disk: 70GB for all VMs
# - Secondary disk: 250GB for all workers and GPU workers
# - Control planes: No secondary disk
#
# Storage Per-VM Customization:
# - Set storage_os_override and storage_data_override to use different storage
# - Leave empty "" to use defaults from proxmox_servers
# - Example: storage_data_override = "nvme-pool"
#
# Longhorn Setup:
# - Secondary disks mounted at: /var/mnt/longhorn_sdb
# - Kubelet mount: /var/lib/longhorn -> /var/mnt/longhorn_sdb
# - All workers and GPU workers get Longhorn storage by default
#
# GPU Passthrough:
# - GPU workers use Proxmox mapped resource "nvidia-gpu-1"
# - Configure in Proxmox UI: Datacenter â†’ Resource Mappings
# - See GPU-ISO-SETUP.md for full instructions
#
# ISOs:
# - Generate standard ISO in Omni UI with extensions:
#   * qemu-guest-agent, nfsd, iscsi-tools, util-linux-tools
# - Generate GPU ISO in Omni UI with same + NVIDIA extensions:
#   * nonfree-kmod-nvidia-production, nvidia-container-toolkit-production
# - Upload both ISOs to Proxmox as: talos-1.11.5.iso and talos-1.11.5-gpu.iso
#
# =============================================================================
