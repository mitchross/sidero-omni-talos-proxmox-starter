# =============================================================================
# Proxmox Servers Configuration
# =============================================================================

proxmox_servers = {
  # First Proxmox server
  "pve1" = {
    api_url          = "https://192.168.10.160:8006/api2/json"
    api_token_id     = "terraform@pve!terraform"
    api_token_secret = "your-secret-here"
    node_name        = "pve1"
    tls_insecure     = true  # Set to false if using valid SSL certificates

    # Storage configuration per VM type
    # List your storage pools: pvesm status (on Proxmox)
    storage_control_plane_os   = "local-lvm"  # Default OS storage for control planes
    storage_control_plane_data = ""           # Default data storage for control planes (empty = no data disk)
    storage_worker_os          = "local-lvm"  # Default OS storage for workers
    storage_worker_data        = "local-lvm"  # Default data storage for workers
    storage_gpu_worker_os      = ""           # Default OS storage for GPU workers (empty = use worker_os)
    storage_gpu_worker_data    = ""           # Default data storage for GPU workers (empty = use worker_data)
    network_bridge             = "vmbr0"      # Network bridge
  }

  # Second Proxmox server (optional)
  # "pve2" = {
  #   api_url                    = "https://192.168.10.161:8006/api2/json"
  #   api_token_id               = "terraform@pve!terraform"
  #   api_token_secret           = "your-secret-here"
  #   node_name                  = "pve2"
  #   tls_insecure               = true
  #   storage_control_plane_os   = "local-lvm"
  #   storage_control_plane_data = ""
  #   storage_worker_os          = "nvme-pool"  # Workers on faster storage
  #   storage_worker_data        = "nvme-pool"
  #   storage_gpu_worker_os      = ""           # Falls back to worker_os
  #   storage_gpu_worker_data    = ""
  #   network_bridge             = "vmbr0"
  # }

  # Third Proxmox server (optional)
  # "pve3" = {
  #   api_url                    = "https://192.168.10.162:8006/api2/json"
  #   api_token_id               = "terraform@pve!terraform"
  #   api_token_secret           = "your-secret-here"
  #   node_name                  = "pve3"
  #   tls_insecure               = true
  #   storage_control_plane_os   = "local-lvm"
  #   storage_control_plane_data = ""
  #   storage_worker_os          = "ceph-pool"  # Workers on Ceph storage
  #   storage_worker_data        = "ceph-pool"
  #   storage_gpu_worker_os      = ""
  #   storage_gpu_worker_data    = ""
  #   network_bridge             = "vmbr0"
  # }
}

# =============================================================================
# Network Configuration
# =============================================================================

network_config = {
  subnet      = "192.168.10.0/24"
  gateway     = "192.168.10.1"
  dns_servers = ["1.1.1.1", "8.8.8.8"]
  vlan_id     = 0  # Set to 0 for no VLAN, or specify VLAN ID
}

# =============================================================================
# Talos Configuration
# =============================================================================

# Boot Method Selection
# ---------------------
# Choose how VMs boot Talos:
#
# "iso" - Boot from ISO file (good for manual setup, testing)
#   Requires: talos_iso uploaded to Proxmox
#   VMs boot into Talos maintenance mode from ISO
#
# "pxe" - Boot from network via PXE (recommended for automation)
#   Requires: Sidero Booter running and DHCP configured for PXE
#   VMs boot from network and pull Talos image from Booter
#
boot_method = "pxe"  # Change to "iso" if you want ISO boot

# ISO Configuration (only needed if boot_method = "iso")
talos_iso = "local:iso/talos-amd64.iso"  # Talos ISO in Proxmox storage

# How to get Talos ISO (for ISO boot method):
# 1. Go to https://factory.talos.dev or https://github.com/siderolabs/talos/releases
# 2. Download metal-amd64.iso (or use Image Factory for custom extensions)
# 3. Upload to Proxmox: Datacenter → Storage (local) → ISO Images → Upload
# 4. Set talos_iso to: "local:iso/metal-amd64.iso" (or your storage:iso/filename)
#
# For GPU workers, you'll need a custom ISO with NVIDIA extensions.
# See: deployment-methods/iso-templates/schematics/gpu-worker.yaml

# General Configuration
talos_version = "v1.10.1"      # Talos version for documentation
cluster_name  = "talos-cluster" # Cluster name

# =============================================================================
# Control Plane Nodes
# =============================================================================
# IMPORTANT: For quorum, deploy odd number of control planes (1, 3, 5, 7)
# Recommendation: At least 1 control plane per Proxmox server

control_planes = [
  {
    name                  = "talos-cp-1"
    proxmox_server        = "pve1"
    ip_address            = "192.168.10.100"
    mac_address           = ""    # Leave empty for auto-generation
    cpu_cores             = 4
    memory_mb             = 8192
    os_disk_size_gb       = 50
    data_disk_size_gb     = 100   # Set to 0 for no data disk
    storage_os_override   = ""    # Optional: Override storage pool for OS disk
    storage_data_override = ""    # Optional: Override storage pool for data disk
  },
  {
    name                  = "talos-cp-2"
    proxmox_server        = "pve1"  # Change to pve2 if you have multiple servers
    ip_address            = "192.168.10.101"
    mac_address           = ""
    cpu_cores             = 4
    memory_mb             = 8192
    os_disk_size_gb       = 50
    data_disk_size_gb     = 100
    storage_os_override   = ""
    storage_data_override = ""
  },
  {
    name                  = "talos-cp-3"
    proxmox_server        = "pve1"  # Change to pve3 if you have multiple servers
    ip_address            = "192.168.10.102"
    mac_address           = ""
    cpu_cores             = 4
    memory_mb             = 8192
    os_disk_size_gb       = 50
    data_disk_size_gb     = 100
    storage_os_override   = ""
    storage_data_override = ""
  },
]

# =============================================================================
# Worker Nodes
# =============================================================================

workers = [
  {
    name                  = "talos-worker-1"
    proxmox_server        = "pve1"
    ip_address            = "192.168.10.110"
    mac_address           = ""
    cpu_cores             = 8
    memory_mb             = 16384
    os_disk_size_gb       = 100
    data_disk_size_gb     = 0     # No data disk for this worker
    storage_os_override   = ""    # Optional: Override storage pool for OS disk
    storage_data_override = ""    # Optional: Override storage pool for data disk
  },
  {
    name                  = "talos-worker-2"
    proxmox_server        = "pve1"
    ip_address            = "192.168.10.111"
    mac_address           = ""
    cpu_cores             = 8
    memory_mb             = 16384
    os_disk_size_gb       = 100
    data_disk_size_gb     = 200   # 200GB data disk for Longhorn
    storage_os_override   = ""
    storage_data_override = ""
  },
  {
    name                  = "talos-worker-3"
    proxmox_server        = "pve1"
    ip_address            = "192.168.10.112"
    mac_address           = ""
    cpu_cores             = 8
    memory_mb             = 16384
    os_disk_size_gb       = 100
    data_disk_size_gb     = 200
    storage_os_override   = ""
    storage_data_override = ""
  },
]

# =============================================================================
# GPU Worker Nodes
# =============================================================================

gpu_workers = [
  # Example GPU worker - uncomment and configure as needed
  # {
  #   name                  = "talos-gpu-1"
  #   proxmox_server        = "pve2"  # Server with GPU
  #   ip_address            = "192.168.10.120"
  #   mac_address           = ""
  #   cpu_cores             = 16
  #   memory_mb             = 32768
  #   os_disk_size_gb       = 100
  #   data_disk_size_gb     = 500     # Large disk for AI/ML workloads
  #   gpu_pci_id            = "01:00" # Find with: lspci | grep -i nvidia
  #   storage_os_override   = ""      # Optional: Override storage pool for OS disk
  #   storage_data_override = ""      # Optional: Override storage pool for data disk
  # },
]

# =============================================================================
# Advanced Options
# =============================================================================

mac_address_prefix = "BC:24:11"  # Prefix for auto-generated MAC addresses
vm_start_on_boot   = true        # Start VMs on Proxmox boot
vm_qemu_agent      = true        # QEMU guest agent (provides VM info to Proxmox via qemu-guest-agent extension)

# =============================================================================
# Quick Start Examples
# =============================================================================

# Example 1: Single Proxmox server, 3 control planes, 3 workers
# - Use configuration above as-is
# - All VMs on pve1

# Example 2: 3 Proxmox servers, distributed control planes
# - Uncomment pve2 and pve3 in proxmox_servers
# - Set control_planes[0].proxmox_server = "pve1"
# - Set control_planes[1].proxmox_server = "pve2"
# - Set control_planes[2].proxmox_server = "pve3"
# - Distribute workers across servers as desired

# Example 3: GPU workloads
# - Uncomment gpu_workers section
# - Set correct proxmox_server (server with GPU)
# - Find GPU PCI ID: ssh to proxmox, run `lspci | grep -i nvidia`
# - After terraform apply, manually configure GPU passthrough in Proxmox UI

# =============================================================================
# How to Get Proxmox API Token
# =============================================================================

# 1. Log into Proxmox web UI
# 2. Navigate to: Datacenter → Permissions → API Tokens
# 3. Create new token:
#    - User: terraform@pve
#    - Token ID: terraform
#    - Uncheck "Privilege Separation"
# 4. Copy the token secret (shown only once!)
# 5. Use format: terraform@pve!terraform for api_token_id
