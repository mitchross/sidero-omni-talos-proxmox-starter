# =============================================================================
# Proxmox Servers Configuration
# =============================================================================

proxmox_servers = {
  # First Proxmox server
  "pve1" = {
    api_url          = "https://192.168.10.160:8006/api2/json"
    api_token_id     = "terraform@pve!terraform"
    api_token_secret = "your-secret-here"
    node_name        = "pve1"
    tls_insecure     = true  # Set to false if using valid SSL certificates

    # Storage configuration for this server
    storage_os   = "local-lvm"    # Storage pool for OS disks
    storage_data = "local-lvm"    # Storage pool for data disks
    network_bridge = "vmbr0"       # Network bridge
  }

  # Second Proxmox server (optional)
  # "pve2" = {
  #   api_url          = "https://192.168.10.161:8006/api2/json"
  #   api_token_id     = "terraform@pve!terraform"
  #   api_token_secret = "your-secret-here"
  #   node_name        = "pve2"
  #   tls_insecure     = true
  #   storage_os       = "local-lvm"
  #   storage_data     = "nvme-pool"  # Different storage for data
  #   network_bridge   = "vmbr0"
  # }

  # Third Proxmox server (optional)
  # "pve3" = {
  #   api_url          = "https://192.168.10.162:8006/api2/json"
  #   api_token_id     = "terraform@pve!terraform"
  #   api_token_secret = "your-secret-here"
  #   node_name        = "pve3"
  #   tls_insecure     = true
  #   storage_os       = "local-lvm"
  #   storage_data     = "ceph-pool"  # Ceph storage for data
  #   network_bridge   = "vmbr0"
  # }
}

# =============================================================================
# Network Configuration
# =============================================================================

network_config = {
  subnet      = "192.168.10.0/24"
  gateway     = "192.168.10.1"
  dns_servers = ["1.1.1.1", "8.8.8.8"]
  vlan_id     = 0  # Set to 0 for no VLAN, or specify VLAN ID
}

# =============================================================================
# Talos Configuration
# =============================================================================

# Boot Method Selection
# ---------------------
# Choose how VMs boot Talos:
#
# "iso" - Boot from ISO file (good for manual setup, testing)
#   Requires: talos_iso uploaded to Proxmox
#   VMs boot into Talos maintenance mode from ISO
#
# "pxe" - Boot from network via PXE (recommended for automation)
#   Requires: Sidero Booter running and DHCP configured for PXE
#   VMs boot from network and pull Talos image from Booter
#
boot_method = "pxe"  # Change to "iso" if you want ISO boot

# ISO Configuration (only needed if boot_method = "iso")
talos_iso = "local:iso/talos-amd64.iso"  # Talos ISO in Proxmox storage

# How to get Talos ISO (for ISO boot method):
# 1. Go to https://factory.talos.dev or https://github.com/siderolabs/talos/releases
# 2. Download metal-amd64.iso (or use Image Factory for custom extensions)
# 3. Upload to Proxmox: Datacenter → Storage (local) → ISO Images → Upload
# 4. Set talos_iso to: "local:iso/metal-amd64.iso" (or your storage:iso/filename)
#
# For GPU workers, you'll need a custom ISO with NVIDIA extensions.
# See: deployment-methods/iso-templates/schematics/gpu-worker.yaml

# General Configuration
talos_version = "v1.10.1"      # Talos version for documentation
cluster_name  = "talos-cluster" # Cluster name

# =============================================================================
# Control Plane Nodes
# =============================================================================
# IMPORTANT: For quorum, deploy odd number of control planes (1, 3, 5, 7)
# Recommendation: At least 1 control plane per Proxmox server

control_planes = [
  {
    name              = "talos-cp-1"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.100"
    mac_address       = ""  # Leave empty for auto-generation
    cpu_cores         = 4
    memory_mb         = 8192
    os_disk_size_gb   = 50
    data_disk_size_gb = 100  # Set to 0 for no data disk
  },
  {
    name              = "talos-cp-2"
    proxmox_server    = "pve1"  # Change to pve2 if you have multiple servers
    ip_address        = "192.168.10.101"
    mac_address       = ""
    cpu_cores         = 4
    memory_mb         = 8192
    os_disk_size_gb   = 50
    data_disk_size_gb = 100
  },
  {
    name              = "talos-cp-3"
    proxmox_server    = "pve1"  # Change to pve3 if you have multiple servers
    ip_address        = "192.168.10.102"
    mac_address       = ""
    cpu_cores         = 4
    memory_mb         = 8192
    os_disk_size_gb   = 50
    data_disk_size_gb = 100
  },
]

# =============================================================================
# Worker Nodes
# =============================================================================

workers = [
  {
    name              = "talos-worker-1"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.110"
    mac_address       = ""
    cpu_cores         = 8
    memory_mb         = 16384
    os_disk_size_gb   = 100
    data_disk_size_gb = 0  # No data disk for this worker
  },
  {
    name              = "talos-worker-2"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.111"
    mac_address       = ""
    cpu_cores         = 8
    memory_mb         = 16384
    os_disk_size_gb   = 100
    data_disk_size_gb = 200  # 200GB data disk for Longhorn
  },
  {
    name              = "talos-worker-3"
    proxmox_server    = "pve1"
    ip_address        = "192.168.10.112"
    mac_address       = ""
    cpu_cores         = 8
    memory_mb         = 16384
    os_disk_size_gb   = 100
    data_disk_size_gb = 200
  },
]

# =============================================================================
# GPU Worker Nodes
# =============================================================================

gpu_workers = [
  # Example GPU worker - uncomment and configure as needed
  # {
  #   name              = "talos-gpu-1"
  #   proxmox_server    = "pve2"  # Server with GPU
  #   ip_address        = "192.168.10.120"
  #   mac_address       = ""
  #   cpu_cores         = 16
  #   memory_mb         = 32768
  #   os_disk_size_gb   = 100
  #   data_disk_size_gb = 500  # Large disk for AI/ML workloads
  #   gpu_pci_id        = "01:00"  # Find with: lspci | grep -i nvidia
  # },
]

# =============================================================================
# Advanced Options
# =============================================================================

mac_address_prefix = "BC:24:11"  # Prefix for auto-generated MAC addresses
vm_start_on_boot   = true        # Start VMs on Proxmox boot
vm_qemu_agent      = false       # QEMU agent (not used by Talos)

# =============================================================================
# Quick Start Examples
# =============================================================================

# Example 1: Single Proxmox server, 3 control planes, 3 workers
# - Use configuration above as-is
# - All VMs on pve1

# Example 2: 3 Proxmox servers, distributed control planes
# - Uncomment pve2 and pve3 in proxmox_servers
# - Set control_planes[0].proxmox_server = "pve1"
# - Set control_planes[1].proxmox_server = "pve2"
# - Set control_planes[2].proxmox_server = "pve3"
# - Distribute workers across servers as desired

# Example 3: GPU workloads
# - Uncomment gpu_workers section
# - Set correct proxmox_server (server with GPU)
# - Find GPU PCI ID: ssh to proxmox, run `lspci | grep -i nvidia`
# - After terraform apply, manually configure GPU passthrough in Proxmox UI

# =============================================================================
# How to Get Proxmox API Token
# =============================================================================

# 1. Log into Proxmox web UI
# 2. Navigate to: Datacenter → Permissions → API Tokens
# 3. Create new token:
#    - User: terraform@pve
#    - Token ID: terraform
#    - Uncheck "Privilege Separation"
# 4. Copy the token secret (shown only once!)
# 5. Use format: terraform@pve!terraform for api_token_id
