# Sidero Omni + Talos on Proxmox Starter Kit

A complete, production-ready starter kit for deploying self-hosted Sidero Omni with the Proxmox infrastructure provider to automatically provision Talos Linux clusters.

## ğŸ“º Video Tutorial: Part 3

[![Watch the Video](https://img.youtube.com/vi/PxnzfzkU6OU/maxresdefault.jpg)](https://www.youtube.com/watch?v=PxnzfzkU6OU)

**Welcome to Part 3 of the Ultimate Kubernetes HomeLab series!**

In this video, weâ€™re diving deep into **Sidero Omni**, the **Omni Proxmox Infra Provider**, and deploying a full **Talos OS cluster from scratch**â€”all running fully self-hosted.

Youâ€™ll learn how to set up Omni, connect it to Proxmox using the official infra provider, provision Talos machines, and manage the entire lifecycle of your Kubernetes nodes through a clean, modern UI.

### ğŸš€ What Youâ€™ll Learn
- Installing *Omni* from the self-hosted GitHub repo
- Deploying the *Sidero Proxmox Infra Provider*
- Configuring Proxmox to work with Omni
- Creating Talos machine classes and templates
- Provisioning Talos nodes automatically on Proxmox
- Bootstrapping & lifecycle management workflows
- Best practices for scaling your Talos cluster

### ğŸ”— Video Series
- **Part 1**: [Complete Kubernetes GitOps Setup Guide: Argo CD, Cilium, K3s](https://www.youtube.com/watch?v=Jm0R74_RkUo)
- **Part 2**: [Complete Kubernetes GitOps Setup Guide Part 2: Talos OS, Proxmox, Terraform](https://www.youtube.com/watch?v=F-qV_n0xW-o)
- **Part 3 (This Video)**: [Sidero Omni + Talos on Proxmox](https://www.youtube.com/watch?v=PxnzfzkU6OU)

### ğŸ“¦ Resources
- **Omni**: [https://github.com/siderolabs/omni](https://github.com/siderolabs/omni)
- **Proxmox Infra Provider**: [https://github.com/siderolabs/omni-infra-provider-proxmox](https://github.com/siderolabs/omni-infra-provider-proxmox)
- **Reference Guide**: [VirtualizationHowTo Guide](https://www.virtualizationhowto.com/2025/08/how-to-install-talos-omni-on-prem-for-effortless-kubernetes-management/)

---

## What This Provides

- **Self-hosted Omni deployment** - Run your own Omni instance on-premises
- **Proxmox integration** - Automatically provision Talos VMs in your Proxmox cluster
- **GPU support** (optional) - Configure NVIDIA GPU passthrough for AI/ML workloads
- **Complete examples** - Working configurations you can customize
- **Setup automation** - Scripts to streamline SSL and encryption setup

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Your Infrastructure                   â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ Omni Server  â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”¤ Proxmox Infrastructure  â”‚  â”‚
â”‚  â”‚ (Self-hosted)â”‚         â”‚ Provider (Docker)       â”‚  â”‚
â”‚  â”‚              â”‚         â”‚                         â”‚  â”‚
â”‚  â”‚ - Web UI     â”‚         â”‚ - Watches Omni API     â”‚  â”‚
â”‚  â”‚ - API        â”‚         â”‚ - Creates VMs          â”‚  â”‚
â”‚  â”‚ - SideroLink â”‚         â”‚ - Manages lifecycle    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚                            â”‚                  â”‚
â”‚         â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”            â”‚
â”‚         â”‚         â”‚   Proxmox Cluster      â”‚            â”‚
â”‚         â”‚         â”‚                        â”‚            â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚            â”‚
â”‚                   â”‚  â”‚ Talos VM Node 1  â”‚  â”‚            â”‚
â”‚                   â”‚  â”‚ Talos VM Node 2  â”‚  â”‚            â”‚
â”‚                   â”‚  â”‚ Talos VM Node 3  â”‚  â”‚            â”‚
â”‚                   â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚            â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

1. **Prerequisites** - See [docs/PREREQUISITES.md](docs/PREREQUISITES.md)
2. **Deploy Omni** - Follow [omni/README.md](omni/README.md)
3. **Setup Provider** - Follow [proxmox-provider/README.md](proxmox-provider/README.md)
4. **Apply Machine Classes** - `omnictl apply -f machine-classes/control-plane.yaml` (repeat for all files)
5. **Sync Cluster Template** - `cd cluster-template && omnictl cluster template sync -v -f cluster-template.yaml`
6. **Create Clusters** - Follow the [Operations Guide](docs/OPERATIONS.md)

## Project Structure

```
.
â”œâ”€â”€ omni/                      # Self-hosted Omni deployment
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ omni.env.example
â”‚   â””â”€â”€ scripts/               # SSL and GPG setup automation
â”œâ”€â”€ proxmox-provider/          # Proxmox infrastructure provider
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ .env.example
â”‚   â””â”€â”€ config.yaml.example
â”œâ”€â”€ talos-configs/             # Example Talos configurations
â”‚   â””â”€â”€ gpu-worker-patch.yaml  # NVIDIA GPU support
â”œâ”€â”€ examples/                  # Complete deployment examples
â”‚   â”œâ”€â”€ simple-homelab/        # Minimal 3-node cluster
â”‚   â”œâ”€â”€ gpu-ml-cluster/        # GPU-enabled for AI/ML
â”‚   â””â”€â”€ production-ha/         # HA cluster with Cilium CNI
â””â”€â”€ docs/                      # Additional documentation
    â”œâ”€â”€ ARCHITECTURE.md
    â”œâ”€â”€ PREREQUISITES.md
    â””â”€â”€ TROUBLESHOOTING.md
```

## Key Features

### Automated Provisioning
Define "machine classes" in Omni that specify CPU, RAM, and disk resources. The Proxmox provider watches for new machines and automatically creates VMs matching your specifications.

### Dual Network Architecture (Management + Storage)
Separate networks for optimal performance:
- **Management Network (vmbr0/ens18)**: DHCP, cluster communication, SideroLink
- **Storage Network (vmbr1/ens19)**: Static IPs, 10G DAC to TrueNAS/NAS

### Storage Pool Separation
Control planes and workers use different Proxmox storage pools:
- **Control Planes**: `fastpool` (high-IOPS for etcd)
- **Workers**: `ssdpool` (balanced performance)

### GPU Support (Optional)
Include NVIDIA GPU support for AI/ML workloads with optimized VM settings:
- CPU passthrough (`cpu=host`)
- Q35 machine type for PCIe passthrough
- NUMA awareness for multi-socket systems
- 1GB hugepages for performance

See [talos-configs/README.md](talos-configs/README.md) for configuration details.

### Production Ready
- SSL/TLS encryption with Let's Encrypt
- Etcd data encryption with GPG  
- SQLite storage backend (Omni v1.4.0+)
- Auth0, SAML, or OIDC authentication
- High availability support



## Advanced Networking

### Cilium CNI

For production deployments, we recommend Cilium CNI:
- **10-40% better performance** vs traditional CNIs
- **eBPF-based** load balancing (replaces kube-proxy)
- **Gateway API** support with advanced routing
- **L3-L7 network policies** for security
- **Hubble** for deep network observability
- **Service mesh** capabilities without sidecars

See the official documentation: [https://docs.cilium.io](https://docs.cilium.io)

**Quick Install**:
```bash
# Disable kube-proxy in cluster config, then:
kubectl apply -f https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.0.0/standard-install.yaml

cilium install \
    --set ipam.mode=kubernetes \
    --set kubeProxyReplacement=true \
    --set securityContext.capabilities.ciliumAgent="{CHOWN,KILL,NET_ADMIN,NET_RAW,IPC_LOCK,SYS_ADMIN,SYS_RESOURCE,DAC_OVERRIDE,FOWNER,SETGID,SETUID}" \
    --set securityContext.capabilities.cleanCiliumState="{NET_ADMIN,SYS_ADMIN,SYS_RESOURCE}" \
    --set cgroup.autoMount.enabled=false \
    --set cgroup.hostRoot=/sys/fs/cgroup \
    --set k8sServiceHost=localhost \
    --set k8sServicePort=7445 \
    --set gatewayAPI.enabled=true \
    --set gatewayAPI.enableAlpn=true \
    --set gatewayAPI.enableAppProtocol=true
```

## Important Notes

âš ï¸ **Proxmox Provider Status**: The Proxmox infrastructure provider is currently in **beta**. Expect some limitations and potential bugs. Please report issues to the [upstream repository](https://github.com/siderolabs/omni-infra-provider-proxmox).

âš ï¸ **Known Limitations**:
- Extensions must be included in Talos image or specified in cluster template

## Use Cases

- **Homelab**: Self-hosted Kubernetes cluster management
- **Edge Computing**: Manage distributed Talos clusters
- **Development**: Rapid cluster provisioning for testing
- **Production**: Enterprise-grade cluster lifecycle management

## License

This starter kit is provided as-is for use with Sidero Omni. Note that:
- Omni uses Business Source License (BSL) - free for non-production use
- Talos Linux is MPL-2.0 licensed
- Proxmox provider is MPL-2.0 licensed

## Contributing

Found a bug? Have an enhancement? PRs welcome! This is a community-driven starter kit.

## Resources

- [Omni Documentation](https://docs.siderolabs.com/omni/)
- [Talos Documentation](https://docs.siderolabs.com/talos/)
- [Proxmox Provider](https://github.com/siderolabs/omni-infra-provider-proxmox)
- [Sidero Labs Slack](https://slack.dev.talos-systems.io/)

## Credits

Built by the community, for the community. Special thanks to the Sidero Labs team for their support and tooling.
